# llm_agents/agents/exploiter.py

from typing import Dict, List
from openai import OpenAI
import os
import json
import re
from jsonschema import validate, ValidationError

# Define JSON schema for exploit plans
EXPLOIT_SCHEMA = {
    "type": "object",
    "properties": {
        "exploit_plan": {
            "type": "object",
            "properties": {
                "setup_steps": {"type": "array", "items": {"type": "string"}},
                "execution_steps": {"type": "array", "items": {"type": "string"}},
                "validation_steps": {"type": "array", "items": {"type": "string"}},
            },
            "required": ["setup_steps", "execution_steps", "validation_steps"],
        }
    },
    "required": ["exploit_plan"],
}


class ExploiterAgent:
    def __init__(self):
        self.client = OpenAI(api_key=os.getenv("OPENAI_API_KEY"))

    def generate_exploit_plan(self, vulnerability_info: Dict) -> Dict:
        """
        Generate an exploit plan for a given vulnerability.

        Args:
            vulnerability_info (Dict): Information about the detected vulnerability.

        Returns:
            Dict: A structured exploit plan with setup, execution, and validation steps.
        """
        system_prompt = """You are a smart contract security expert. Given a specific vulnerability in a smart contract, generate a detailed exploit plan.

                                                    Ensure the response is strictly in JSON format as follows:

                                                    {
                                                        "exploit_plan": {
                                                            "setup_steps": ["Step 1", "Step 2", ...],
                                                            "execution_steps": ["Step 1", "Step 2", ...],
                                                            "validation_steps": ["Step 1", "Step 2", ...]
                                                        }
                                                    }

                                                    Do not include any additional text outside of this JSON structure."""
        prompt = self._construct_exploit_prompt(vulnerability_info)

        response = self.client.chat.completions.create(
            model="o1-mini",
            messages=[
                # {
                #     "role": "system",
                #     "content": system_prompt,
                # },
                {"role": "user", "content": system_prompt + prompt},
            ],
            temperature=0,
        )

        exploit_plan = self._parse_exploit_response(response.choices[0].message.content)

        return {"exploit_plan": exploit_plan}

    def _construct_exploit_prompt(self, vulnerability_info: Dict) -> str:
        """
        Constructs the prompt for the LLM based on vulnerability information.

        Args:
            vulnerability_info (Dict): Information about the detected vulnerability.

        Returns:
            str: The constructed prompt.
        """
        prompt = f"""
                Analyze the following vulnerability and generate a detailed exploit plan.

                Vulnerability Type: {vulnerability_info.get('vulnerability_type', 'N/A')}
                Confidence Score: {vulnerability_info.get('confidence_score', 'N/A')}
                Reasoning: {vulnerability_info.get('reasoning', 'N/A')}
                Affected Functions: {', '.join(vulnerability_info.get('affected_functions', []))}

                Provide the exploit plan in the following JSON format:

                {{
                    "exploit_plan": {{
                        "setup_steps": ["Step 1", "Step 2", ...],
                        "execution_steps": ["Step 1", "Step 2", ...],
                        "validation_steps": ["Step 1", "Step 2", ...]
                    }}
                }}
                """
        return prompt

    def _parse_exploit_response(self, response: str) -> Dict:
        """
        Parses the LLM response into a structured exploit plan using JSON schema validation.

        Args:
            response (str): The raw response from the LLM.

        Returns:
            Dict: A structured exploit plan.
        """
        try:
            # Attempt to parse the response as JSON
            parsed_response = json.loads(response)
            # Validate against schema
            validate(instance=parsed_response, schema=EXPLOIT_SCHEMA)
            exploit_plan = parsed_response.get("exploit_plan", {})
            return exploit_plan
        except json.JSONDecodeError:
            # Attempt to extract JSON from the response
            json_pattern = re.compile(r"\{.*\}", re.DOTALL)
            match = json_pattern.search(response)
            if match:
                try:
                    parsed_response = json.loads(match.group())
                    validate(instance=parsed_response, schema=EXPLOIT_SCHEMA)
                    exploit_plan = parsed_response.get("exploit_plan", {})
                    return exploit_plan
                except (json.JSONDecodeError, ValidationError):
                    pass
            # Fallback in case of parsing errors
            print("Failed to parse exploit plan response.")
            return {"setup_steps": [], "execution_steps": [], "validation_steps": []}
