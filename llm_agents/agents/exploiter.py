# llm_agents/agents/exploiter.py

from typing import Dict, List
from openai import OpenAI
import os
import json
import re
from jsonschema import validate, ValidationError

# Define JSON schema for exploit plans
EXPLOIT_SCHEMA = {
    "type": "object",
    "properties": {
        "plan": {
            "type": "object",
            "properties": {
                "setup_steps": {"type": "array", "items": {"type": "string"}},
                "execution_steps": {"type": "array", "items": {"type": "string"}},
                "validation_steps": {"type": "array", "items": {"type": "string"}},
            },
            "required": ["setup_steps", "execution_steps", "validation_steps"],
        }
    },
    "required": ["plan"],
}


class ExploiterAgent:
    def __init__(self):
        self.client = OpenAI(api_key=os.getenv("OPENAI_API_KEY"))

    def generate_exploit_plan(self, vulnerability_info: Dict) -> Dict:
        """
        Generate an exploit plan for a given vulnerability.

        Args:
            vulnerability_info (Dict): Information about the detected vulnerability.

        Returns:
            Dict: A structured exploit plan with setup, execution, and validation steps.
        """
        system_prompt = """You are a smart contract security expert. Given a specific vulnerability in a smart contract, your job is to give a detailed PoC to demonstrate it."""
        prompt = self._construct_exploit_prompt(vulnerability_info)

        response = self.client.chat.completions.create(
            model="o1-mini",
            messages=[
                # {
                #     "role": "system",
                #     "content": system_prompt,
                # },
                {"role": "user", "content": system_prompt + prompt},
            ],
            # temperature=0,
        )

        exploit_plan = self._parse_exploit_response(response.choices[0].message.content)

        return {"exploit_plan": exploit_plan}

    def _construct_exploit_prompt(self, vulnerability_info: Dict) -> str:
        """
        Constructs the prompt for the LLM based on vulnerability information.

        Args:
            vulnerability_info (Dict): Information about the detected vulnerability.

        Returns:
            str: The constructed prompt.
        """
        prompt = f"""
                Analyze the following vulnerability and generate a detailed plan to demonstrate it.

                Vulnerability Type: {vulnerability_info.get('vulnerability_type', 'N/A')}
                Confidence Score: {vulnerability_info.get('confidence_score', 'N/A')}
                Reasoning: {vulnerability_info.get('reasoning', 'N/A')}
                Affected Functions: {', '.join(vulnerability_info.get('affected_functions', []))}
                Code Snippet: {vulnerability_info.get('code_snippet', 'N/A')}

                Provide the exploit plan in the following JSON format:

                {{
                    "plan": {{
                        "setup_steps": ["Step 1", "Step 2", ...],
                        "execution_steps": ["Step 1", "Step 2", ...],
                        "validation_steps": ["Step 1", "Step 2", ...]
                    }}
                }}
                """
        return prompt

    def _parse_exploit_response(self, response: str) -> Dict:
        """
        Parses the LLM response into a structured exploit plan using JSON schema validation.

        Args:
            response (str): The raw response from the LLM.

        Returns:
            Dict: A structured exploit plan.
        """
        print("LLM Response:", response)
        try:
            # Attempt to parse the response as JSON
            parsed_response = json.loads(response)
            # Validate against schema
            validate(instance=parsed_response, schema=EXPLOIT_SCHEMA)
            exploit_plan = parsed_response.get("plan", {})
            return exploit_plan
        except json.JSONDecodeError:
            # Attempt to extract JSON from the response
            match = re.search(r"```json\s+(.*?)\s+```", response, re.DOTALL)

            if match:
                try:
                    json_str = match.group(1).strip()
                    json_str = re.sub(
                        r"\n\s+", " ", json_str
                    )  # Remove unexpected newlines in values
                    print("Exploit:", json_str)
                    parsed_response = json.loads(json_str)
                    validate(instance=parsed_response, schema=EXPLOIT_SCHEMA)
                    exploit_plan = parsed_response.get("plan", {})
                    return exploit_plan
                except (json.JSONDecodeError, ValidationError):
                    pass
            # Fallback in case of parsing errors
            print("Failed to parse exploit plan response.")
            return {"setup_steps": [], "execution_steps": [], "validation_steps": []}
