#!/usr/bin/env python3
"""
Benchmark Evaluator

This script compares vulnerability reports generated by the system against ground truth
synopsis files from the benchmark_data directory. It calculates precision, recall, and
F1 scores for each comparison method and generates visualizations.

The script supports:
1. Evaluation using LLM-based comparison
2. Evaluation using simple string matching
3. Processing multiple report directories in parallel
4. Handling special cases for "no_errors" contracts
"""

import os
import re
import json
import glob
import time
import asyncio
import argparse
from typing import Dict, List, Tuple, Optional, Set, Any
from pathlib import Path
from dotenv.main import load_dotenv
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from openai import AsyncOpenAI
import logging
from datetime import datetime
from collections import defaultdict

from llm_agents.config import ModelConfig

# Configure logging
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler("benchmark_evaluation.log"),
        logging.StreamHandler()
    ]
)
logger = logging.getLogger("benchmark_evaluator")

# Constants
UPLOADS_DIR = "uploads"
BENCHMARK_DATA_DIR = "benchmark_data"
BENCHMARK_DATA_NO_ERRORS_DIR = "benchmark_data_no_errors"
BENCHMARK_RESULTS_DIR = "benchmark_results"

# LLM Evaluation settings
EVAL_MODEL = "o3-mini"  # Model for evaluation
EVAL_MAX_ATTEMPTS = 3  # Max retry attempts for LLM evaluation
EVAL_TEMPERATURE = 0.1  # Temperature for evaluation prompts

# Categories
CATEGORIES = [
    "access_control",
    "arithmetic_security",
    "boundary_condition",
    "cryptoeconomic_security",
    "data_structure_security",
    "gas_security",
    "privacy_crypto_security"
]

class BenchmarkEvaluator:
    """Evaluates vulnerability reports against ground truth synopsis files."""

    def __init__(self,
                 uploads_dir: str = UPLOADS_DIR,
                 benchmark_data_dir: str = BENCHMARK_DATA_DIR,
                 benchmark_no_errors_dir: str = BENCHMARK_DATA_NO_ERRORS_DIR,
                 results_dir: str = BENCHMARK_RESULTS_DIR,
                 use_llm: bool = True,
                 eval_model: str = EVAL_MODEL,
                 max_workers: int = 5):
        """
        Initialize the benchmark evaluator.

        Args:
            uploads_dir: Directory containing generated reports
            benchmark_data_dir: Directory containing benchmark data with errors
            benchmark_no_errors_dir: Directory containing benchmark data with no errors
            results_dir: Directory to save evaluation results
            use_llm: Whether to use LLM-based evaluation
            eval_model: LLM model to use for evaluation
            max_workers: Maximum number of parallel workers
        """
        self.uploads_dir = Path(uploads_dir)
        self.benchmark_data_dir = Path(benchmark_data_dir)
        self.benchmark_no_errors_dir = Path(benchmark_no_errors_dir)
        self.results_dir = Path(results_dir)
        self.use_llm = use_llm
        self.eval_model = eval_model
        self.max_workers = max_workers

        # Check if required directories exist
        self._check_directories()

        # Create model config for LLM client
        self.model_config = ModelConfig()

        # Initialize OpenAI client
        self.client = None
        if self.use_llm:
            self._setup_llm_client()

        # Create results directory if it doesn't exist
        os.makedirs(self.results_dir, exist_ok=True)

        # Store evaluation results
        self.results = {}

    def _check_directories(self):
        """Check if required directories exist and create them if necessary."""
        # Check uploads directory
        if not os.path.exists(self.uploads_dir):
            logger.warning(f"Uploads directory {self.uploads_dir} does not exist. Creating it...")
            os.makedirs(self.uploads_dir, exist_ok=True)

        # Check benchmark data directories
        if not os.path.exists(self.benchmark_data_dir):
            logger.error(f"Benchmark data directory {self.benchmark_data_dir} does not exist")
            logger.error("Please ensure the benchmark data is properly set up")

        # Check for synopsis directory
        synopsis_dir = self.benchmark_data_dir / "synopsis"
        if not os.path.exists(synopsis_dir):
            logger.error(f"Synopsis directory {synopsis_dir} does not exist")
            logger.error("Please ensure the benchmark data is properly set up")

        # Check no errors directory
        if not os.path.exists(self.benchmark_no_errors_dir):
            logger.warning(f"No-errors benchmark directory {self.benchmark_no_errors_dir} does not exist")

        # Create results directory
        os.makedirs(self.results_dir, exist_ok=True)

    def _setup_llm_client(self):
        """Set up the LLM client for evaluation."""
        import os

        args = self.model_config.get_openai_args(self.eval_model)
        provider, api_key_env, base_url = self.model_config.get_provider_info(self.eval_model)

        if base_url:
            args["base_url"] = base_url

        # Check if API key is available in environment
        load_dotenv()
        api_key = os.getenv(api_key_env)
        if not api_key:
            logger.warning(f"API key environment variable {api_key_env} not found. LLM evaluation will be disabled.")
            logger.warning(f"Set the {api_key_env} environment variable to enable LLM evaluation.")
            self.use_llm = False
            return

        args["api_key"] = api_key

        try:
            self.client = AsyncOpenAI(**args)
            logger.info(f"LLM client set up with model {self.eval_model}")
        except Exception as e:
            logger.error(f"Failed to initialize LLM client: {e}")
            logger.warning("Falling back to string matching evaluation")
            self.use_llm = False

    async def evaluate_all(self, report_dirs: List[str] = None) -> Dict:
        """
        Evaluate all reports across specified directories.

        Args:
            report_dirs: List of report directories to evaluate (if None, find all directories)

        Returns:
            Dict containing evaluation results
        """
        # Check if uploads directory exists
        if not os.path.exists(self.uploads_dir):
            logger.error(f"Uploads directory {self.uploads_dir} does not exist")
            return {}

        # If no report dirs provided, find all subdirectories in uploads
        if not report_dirs:
            report_dirs = [d.name for d in self.uploads_dir.iterdir()
                          if d.is_dir() and not d.name.startswith('.')]

        if not report_dirs:
            logger.warning(f"No report directories found in {self.uploads_dir}")
            return {}

        logger.info(f"Found {len(report_dirs)} report directories: {report_dirs}")

        # Process each directory
        all_results = {}
        for report_dir in report_dirs:
            logger.info(f"Evaluating reports in {report_dir}")
            results = await self.evaluate_directory(report_dir)
            all_results[report_dir] = results

        self.results = all_results
        return all_results

    async def evaluate_directory(self, report_dir: str) -> Dict:
        """
        Evaluate all reports in a specific directory.

        Args:
            report_dir: Name of the report directory

        Returns:
            Dict containing evaluation results for the directory
        """
        dir_path = self.uploads_dir / report_dir
        if not dir_path.exists() or not dir_path.is_dir():
            logger.error(f"Directory {dir_path} does not exist")
            return {}

        # Get list of all report files (.md files) in the directory
        report_files = list(dir_path.glob("*.md"))
        logger.info(f"Found {len(report_files)} report files in {report_dir}")

        # Process reports in batches according to max_workers
        results = {}

        # Process with_errors reports
        error_reports = [f for f in report_files if not self._is_no_errors_report(f.name)]
        if error_reports:
            logger.info(f"Processing {len(error_reports)} reports with errors")
            error_tasks = []

            for report_file in error_reports:
                category = self._get_category_from_filename(report_file.name)
                if category:
                    task = self.evaluate_report(report_file, category)
                    error_tasks.append(task)
                else:
                    logger.warning(f"Could not determine category for {report_file.name}")

            error_results = await asyncio.gather(*error_tasks)
            for result in error_results:
                if result:  # Skip None results
                    results[result["category"]] = result

        # Process no_errors reports
        no_error_reports = [f for f in report_files if self._is_no_errors_report(f.name)]
        if no_error_reports:
            logger.info(f"Processing {len(no_error_reports)} reports with no errors")
            no_error_tasks = []

            for report_file in no_error_reports:
                contract_name = self._get_contract_name_from_filename(report_file.name)
                if contract_name:
                    task = self.evaluate_no_errors_report(report_file, contract_name)
                    no_error_tasks.append(task)
                else:
                    logger.warning(f"Could not determine contract name for {report_file.name}")

            no_error_results = await asyncio.gather(*no_error_tasks)
            for result in no_error_results:
                if result:  # Skip None results
                    results[result["contract_name"]] = result

        return results

    async def evaluate_report(self, report_file: Path, category: str) -> Dict:
        """
        Evaluate a single report file against its synopsis.

        Args:
            report_file: Path to the report file
            category: Vulnerability category

        Returns:
            Dict containing evaluation results
        """
        logger.info(f"Evaluating report {report_file.name} for category {category}")

        # Read report content
        try:
            with open(report_file, 'r', encoding='utf-8') as f:
                report_content = f.read().strip()

            # Check if report is empty or too short to be meaningful
            if not report_content or len(report_content) < 50:
                logger.warning(f"Report file {report_file} is empty or too short to analyze")
                return {
                    "category": category,
                    "file": report_file.name,
                    "has_errors": True,
                    "true_positive": 0,
                    "false_positive": 0,
                    "false_negative": 1,  # Assuming we missed a vulnerability since we couldn't compare
                    "precision": 0.0,
                    "recall": 0.0,
                    "f1_score": 0.0,
                    "analysis": "Report file is empty or too short to analyze",
                    "evaluation_method": "error"
                }
        except Exception as e:
            logger.error(f"Error reading report file {report_file}: {e}")
            return None

        # Read synopsis content (ground truth)
        synopsis_path = self.benchmark_data_dir / "synopsis" / f"{category}.txt"
        if not synopsis_path.exists():
            # Try alternate extensions if .txt doesn't exist
            alt_paths = [
                self.benchmark_data_dir / "synopsis" / f"{category}.md",
                self.benchmark_data_dir / "synopsis" / f"{category}"
            ]

            for alt_path in alt_paths:
                if os.path.exists(alt_path):
                    synopsis_path = alt_path
                    break
            else:
                logger.error(f"Synopsis file for {category} does not exist")
                logger.warning(f"Looked for: {synopsis_path} and alternatives")
                # Return a default evaluation with a warning message instead of failing
                return {
                    "category": category,
                    "file": report_file.name,
                    "has_errors": True,
                    "true_positive": 0,
                    "false_positive": 0,
                    "false_negative": 1,  # Assuming we missed a vulnerability since we couldn't compare
                    "precision": 0.0,
                    "recall": 0.0,
                    "f1_score": 0.0,
                    "analysis": f"Unable to evaluate: synopsis file for {category} not found",
                    "evaluation_method": "error"
                }

        try:
            with open(synopsis_path, 'r', encoding='utf-8') as f:
                synopsis_content = f.read().strip()
        except Exception as e:
            logger.error(f"Error reading synopsis file {synopsis_path}: {e}")
            # Return a default evaluation with a warning message instead of failing
            return {
                "category": category,
                "file": report_file.name,
                "has_errors": True,
                "true_positive": 0,
                "false_positive": 0,
                "false_negative": 1,  # Assuming we missed a vulnerability since we couldn't compare
                "precision": 0.0,
                "recall": 0.0,
                "f1_score": 0.0,
                "analysis": f"Unable to evaluate: error reading synopsis file - {str(e)}",
                "evaluation_method": "error"
            }

        # Evaluate using LLM or string matching
        if self.use_llm and self.client:
            evaluation = await self._evaluate_with_llm(report_content, synopsis_content, category)
        else:
            evaluation = self._evaluate_with_string_matching(report_content, synopsis_content)

        # Combine results
        result = {
            "category": category,
            "file": report_file.name,
            "has_errors": True,
            **evaluation
        }

        return result

    async def evaluate_no_errors_report(self, report_file: Path, contract_name: str) -> Dict:
        """
        Evaluate a report for a contract that has no vulnerabilities.

        Args:
            report_file: Path to the report file
            contract_name: Name of the contract

        Returns:
            Dict containing evaluation results
        """
        logger.info(f"Evaluating no-errors report {report_file.name} for contract {contract_name}")

        # Read report content
        try:
            with open(report_file, 'r', encoding='utf-8') as f:
                report_content = f.read().strip()

            # Check if report is empty or too short to be meaningful
            if not report_content or len(report_content) < 50:
                logger.warning(f"Report file {report_file} is empty or too short to analyze")
                return {
                    "contract_name": contract_name,
                    "file": report_file.name,
                    "has_errors": False,
                    "true_positive": 0,
                    "false_positive": 0,
                    "false_negative": 0,
                    "precision": 0.0,
                    "recall": 0.0,
                    "f1_score": 0.0,
                    "analysis": "Report file is empty or too short to analyze",
                    "evaluation_method": "error"
                }
        except Exception as e:
            logger.error(f"Error reading report file {report_file}: {e}")
            return None

        # For no_errors contracts, we need to check if vulnerabilities were reported
        vulnerabilities_reported = self._check_for_reported_vulnerabilities(report_content)

        # Determine evaluation results
        # True negative: Report finds no vulnerabilities in a contract that has none
        # False positive: Report finds vulnerabilities in a contract that has none
        if vulnerabilities_reported:
            # False positive
            result = {
                "contract_name": contract_name,
                "file": report_file.name,
                "has_errors": False,
                "true_positive": 0,
                "false_positive": 1,
                "false_negative": 0,
                "precision": 0.0,
                "recall": 1.0,  # Recall is 1.0 since there are no actual vulnerabilities
                "f1_score": 0.0,
                "evaluation_method": "no_errors_check"
            }
        else:
            # True negative
            result = {
                "contract_name": contract_name,
                "file": report_file.name,
                "has_errors": False,
                "true_positive": 0,
                "false_positive": 0,
                "false_negative": 0,
                "precision": 1.0,  # Perfect precision since no false positives
                "recall": 1.0,     # Perfect recall since no false negatives
                "f1_score": 1.0,   # Perfect F1 score
                "evaluation_method": "no_errors_check"
            }

        return result

    def _check_for_reported_vulnerabilities(self, report_content: str) -> bool:
        """
        Check if a report claims to have found vulnerabilities.

        Args:
            report_content: Content of the report

        Returns:
            True if vulnerabilities were reported, False otherwise
        """
        # Common phrases that indicate no vulnerabilities were found
        no_vulns_phrases = [
            "no vulnerabilities were detected",
            "no vulnerabilities found",
            "no potential vulnerabilities",
            "no security issues",
            "contract is secure",
            "code is secure",
            "security analysis revealed no"
        ]

        # Check for phrases indicating no vulnerabilities
        report_lower = report_content.lower()
        if any(phrase in report_lower for phrase in no_vulns_phrases):
            return False

        # Check for specific vulnerability sections/headings
        vuln_section_patterns = [
            r"##\s+.*vulnerabilit(y|ies)",
            r"###\s+.*vulnerabilit(y|ies)",
            r"critical.*issue",
            r"high.*severity",
            r"medium.*severity"
        ]

        if any(re.search(pattern, report_lower) for pattern in vuln_section_patterns):
            return True

        # Default to assuming vulnerabilities were reported
        # This is more conservative as it penalizes false negatives more than false positives
        return True

    async def _evaluate_with_llm(self, report_content: str, synopsis_content: str, category: str) -> Dict:
        """
        Evaluate a report against a synopsis using an LLM.

        Args:
            report_content: Content of the report
            synopsis_content: Content of the synopsis (ground truth)
            category: Vulnerability category

        Returns:
            Dict containing evaluation results
        """
        if not self.client:
            logger.warning("LLM client not available, falling back to string matching")
            return self._evaluate_with_string_matching(report_content, synopsis_content)

        # Construct the evaluation prompt
        prompt = f"""You are an expert smart contract security auditor tasked with evaluating a vulnerability report against the ground truth.

GROUND TRUTH VULNERABILITY DESCRIPTION:
```
{synopsis_content}
```

VULNERABILITY REPORT TO EVALUATE:
```
{report_content}
```

Determine whether the report correctly identifies the key vulnerabilities described in the ground truth.
Focus on the main vulnerability concepts, not specific wording.

Provide your evaluation in the following JSON format:
{{
    "true_positive": number of correct vulnerabilities found,
    "false_positive": number of incorrectly reported vulnerabilities (not in ground truth),
    "false_negative": number of vulnerabilities in ground truth that were missed,
    "analysis": "brief explanation of your evaluation"
}}
"""

        # Attempt to get response from LLM with retries
        for attempt in range(EVAL_MAX_ATTEMPTS):
            try:
                response = await self.client.chat.completions.create(
                    model=self.eval_model,
                    messages=[{"role": "user", "content": prompt}],
                    # temperature=EVAL_TEMPERATURE,
                    response_format={"type": "json_object"}
                )

                # Parse the JSON response
                json_content = response.choices[0].message.content
                evaluation = json.loads(json_content)

                # Validate evaluation format
                if not all(k in evaluation for k in ["true_positive", "false_positive", "false_negative"]):
                    logger.warning(f"LLM evaluation returned incomplete response: {evaluation}")
                    if attempt < EVAL_MAX_ATTEMPTS - 1:
                        continue
                    else:
                        logger.error("Invalid LLM response format, falling back to string matching")
                        return self._evaluate_with_string_matching(report_content, synopsis_content)

                # Calculate metrics
                tp = int(evaluation.get("true_positive", 0))  # Convert to int to handle string responses
                fp = int(evaluation.get("false_positive", 0))
                fn = int(evaluation.get("false_negative", 0))

                precision = tp / (tp + fp) if (tp + fp) > 0 else 0
                recall = tp / (tp + fn) if (tp + fn) > 0 else 0
                f1_score = 2 * (precision * recall) / (precision + recall) if (precision + recall) > 0 else 0

                result = {
                    "true_positive": tp,
                    "false_positive": fp,
                    "false_negative": fn,
                    "precision": precision,
                    "recall": recall,
                    "f1_score": f1_score,
                    "analysis": evaluation.get("analysis", ""),
                    "evaluation_method": "llm"
                }

                return result

            except Exception as e:
                logger.warning(f"LLM evaluation attempt {attempt+1} failed: {e}")
                if attempt < EVAL_MAX_ATTEMPTS - 1:
                    # Wait before retrying
                    await asyncio.sleep(1 * (attempt + 1))
                else:
                    logger.error(f"LLM evaluation failed after {EVAL_MAX_ATTEMPTS} attempts, falling back to string matching")
                    return self._evaluate_with_string_matching(report_content, synopsis_content)

    def _evaluate_with_string_matching(self, report_content: str, synopsis_content: str) -> Dict:
        """
        Evaluate a report against a synopsis using simple string matching.

        Args:
            report_content: Content of the report
            synopsis_content: Content of the synopsis (ground truth)

        Returns:
            Dict containing evaluation results
        """
        # Extract key terms from the synopsis
        synopsis_terms = self._extract_key_terms(synopsis_content)

        # Check how many terms from the synopsis appear in the report
        matched_terms = [term for term in synopsis_terms if term.lower() in report_content.lower()]

        # Calculate metrics
        true_positive = len(matched_terms)
        false_negative = len(synopsis_terms) - true_positive

        # Rough estimate of false positives
        # Count unique security-related terms in the report that aren't in the synopsis
        report_terms = self._extract_key_terms(report_content)
        false_positive = len([term for term in report_terms
                             if term.lower() not in synopsis_content.lower()
                             and self._is_security_term(term)])

        # Calculate metrics
        precision = true_positive / (true_positive + false_positive) if (true_positive + false_positive) > 0 else 0
        recall = true_positive / (true_positive + false_negative) if (true_positive + false_negative) > 0 else 0
        f1_score = 2 * (precision * recall) / (precision + recall) if (precision + recall) > 0 else 0

        result = {
            "true_positive": true_positive,
            "false_positive": false_positive,
            "false_negative": false_negative,
            "precision": precision,
            "recall": recall,
            "f1_score": f1_score,
            "analysis": f"Matched {true_positive} out of {len(synopsis_terms)} key terms",
            "evaluation_method": "string_matching"
        }

        return result

    def _extract_key_terms(self, text: str) -> List[str]:
        """
        Extract key security-related terms from text.

        Args:
            text: Text to extract terms from

        Returns:
            List of key terms
        """
        # Common security-related terms
        security_terms = [
            "reentrancy", "overflow", "underflow", "access control", "authorization",
            "front-running", "oracle", "price manipulation", "flash loan", "time manipulation",
            "randomness", "denial of service", "dos", "unchecked", "gas", "precision",
            "arithmetic", "rounding", "race condition", "centralization", "governance",
            "privilege", "permission", "authentication", "validation", "boundary", "check"
        ]

        # Find all occurrences of security terms
        found_terms = []
        for term in security_terms:
            if term.lower() in text.lower():
                found_terms.append(term)

        # Also extract function names
        function_pattern = r"\b(function|modifier)\s+(\w+)"
        for match in re.finditer(function_pattern, text):
            found_terms.append(match.group(2))

        return found_terms

    def _is_security_term(self, term: str) -> bool:
        """
        Check if a term is security-related.

        Args:
            term: Term to check

        Returns:
            True if the term is security-related, False otherwise
        """
        security_keywords = [
            "vulnerability", "attack", "exploit", "security", "risk", "issue",
            "problem", "bug", "flaw", "weakness", "threat", "compromise", "breach",
            "critical", "severe", "overflow", "underflow", "reentrancy", "access",
            "control", "authorization", "authentication", "validation", "check"
        ]

        return any(keyword in term.lower() for keyword in security_keywords)

    def _get_category_from_filename(self, filename: str) -> Optional[str]:
        """
        Extract the vulnerability category from a filename.

        Args:
            filename: Name of the file

        Returns:
            Category name or None if cannot be determined
        """
        # Check for direct category in the filename
        for category in CATEGORIES:
            if category in filename.lower():
                return category

        # If no direct match, try to parse from other formats
        # Example: "analysis_report_category_timestamp.md"
        parts = filename.split('_')
        for category in CATEGORIES:
            if category in parts:
                return category

        return None

    def _get_contract_name_from_filename(self, filename: str) -> Optional[str]:
        """
        Extract the contract name from a filename for no_errors contracts.

        Args:
            filename: Name of the file

        Returns:
            Contract name or None if cannot be determined
        """
        # Check for common contract names in the no_errors directory
        contract_names = ["Lending", "MerkleDrop", "Vesting", "Voting"]

        for name in contract_names:
            if name.lower() in filename.lower():
                return name

        return None

    def _is_no_errors_report(self, filename: str) -> bool:
        """
        Check if a filename corresponds to a no_errors report.

        Args:
            filename: Name of the file

        Returns:
            True if the file is a no_errors report, False otherwise
        """
        # Check for common contract names that don't have errors
        contract_names = ["Lending", "MerkleDrop", "Vesting", "Voting"]

        # Check if any contract name is in the filename and not in categories
        return any(name.lower() in filename.lower() for name in contract_names) and \
               not any(category in filename.lower() for category in CATEGORIES)

    def generate_summary(self) -> Dict:
        """
        Generate a summary of the evaluation results.

        Returns:
            Dict containing summary statistics
        """
        if not self.results:
            logger.warning("No results to summarize")
            return {}

        summary = {}

        # Calculate average metrics for each report directory
        for report_dir, results in self.results.items():
            dir_summary = {
                "precision": [],
                "recall": [],
                "f1_score": [],
                "true_positive": 0,
                "false_positive": 0,
                "false_negative": 0,
                "with_errors_count": 0,
                "no_errors_count": 0,
                "evaluation_methods": defaultdict(int)
            }

            for item_key, item_result in results.items():
                dir_summary["precision"].append(item_result.get("precision", 0))
                dir_summary["recall"].append(item_result.get("recall", 0))
                dir_summary["f1_score"].append(item_result.get("f1_score", 0))
                dir_summary["true_positive"] += item_result.get("true_positive", 0)
                dir_summary["false_positive"] += item_result.get("false_positive", 0)
                dir_summary["false_negative"] += item_result.get("false_negative", 0)

                if item_result.get("has_errors", True):
                    dir_summary["with_errors_count"] += 1
                else:
                    dir_summary["no_errors_count"] += 1

                method = item_result.get("evaluation_method", "unknown")
                dir_summary["evaluation_methods"][method] += 1

            # Calculate averages
            dir_summary["avg_precision"] = np.mean(dir_summary["precision"]) if dir_summary["precision"] else 0
            dir_summary["avg_recall"] = np.mean(dir_summary["recall"]) if dir_summary["recall"] else 0
            dir_summary["avg_f1_score"] = np.mean(dir_summary["f1_score"]) if dir_summary["f1_score"] else 0

            # Calculate macro F1
            dir_summary["macro_f1"] = 2 * (dir_summary["avg_precision"] * dir_summary["avg_recall"]) / \
                                     (dir_summary["avg_precision"] + dir_summary["avg_recall"]) \
                                     if (dir_summary["avg_precision"] + dir_summary["avg_recall"]) > 0 else 0

            summary[report_dir] = dir_summary

        return summary

    def save_results(self, filename: str = None) -> str:
        """
        Save evaluation results to a JSON file.

        Args:
            filename: Name of the file to save results to (without extension)

        Returns:
            Path to the saved file
        """
        if not self.results:
            logger.warning("No results to save")
            return ""

        # Generate filename based on timestamp if not provided
        if not filename:
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            filename = f"eval_results_{timestamp}"

        # Ensure .json extension
        if not filename.endswith(".json"):
            filename += ".json"

        # Create path in results directory
        file_path = self.results_dir / filename

        # Save results to JSON file
        try:
            with open(file_path, 'w', encoding='utf-8') as f:
                json.dump(self.results, f, indent=2)
            logger.info(f"Results saved to {file_path}")
            return str(file_path)
        except Exception as e:
            logger.error(f"Error saving results to {file_path}: {e}")
            return ""

    def generate_visualizations(self) -> List[str]:
        """
        Generate visualizations of the evaluation results.

        Returns:
            List of paths to generated visualization files
        """
        if not self.results:
            logger.warning("No results to visualize")
            return []

        # Get summary statistics
        summary = self.generate_summary()

        # Prepare data for visualization
        viz_files = []

        # 1. Bar chart of average metrics by model/configuration
        metrics_file = self._plot_metrics_comparison(summary)
        if metrics_file:
            viz_files.append(metrics_file)

        # 2. Confusion matrix
        conf_matrix_file = self._plot_confusion_matrix(summary)
        if conf_matrix_file:
            viz_files.append(conf_matrix_file)

        # 3. F1 score by category
        f1_by_category_file = self._plot_f1_by_category()
        if f1_by_category_file:
            viz_files.append(f1_by_category_file)

        return viz_files

    def _plot_metrics_comparison(self, summary: Dict) -> str:
        """
        Plot comparison of precision, recall, and F1 scores.

        Args:
            summary: Summary statistics

        Returns:
            Path to the generated plot file
        """
        # Prepare data
        models = []
        precision_vals = []
        recall_vals = []
        f1_vals = []

        for model, stats in summary.items():
            models.append(model)
            precision_vals.append(stats["avg_precision"])
            recall_vals.append(stats["avg_recall"])
            f1_vals.append(stats["avg_f1_score"])

        # Create plot
        plt.figure(figsize=(12, 6))
        bar_width = 0.25
        index = np.arange(len(models))

        plt.bar(index, precision_vals, bar_width, label='Precision', color='#2a9fd6')
        plt.bar(index + bar_width, recall_vals, bar_width, label='Recall', color='#77b300')
        plt.bar(index + 2*bar_width, f1_vals, bar_width, label='F1 Score', color='#9933cc')

        plt.xlabel('Model Configuration')
        plt.ylabel('Score')
        plt.title('Precision, Recall, and F1 Score by Model Configuration')
        plt.xticks(index + bar_width, models, rotation=45, ha='right')
        plt.legend()
        plt.tight_layout()

        # Save plot
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        file_path = self.results_dir / f"metrics_comparison_{timestamp}.png"
        plt.savefig(file_path, dpi=300)
        plt.close()

        return str(file_path)

    def _plot_confusion_matrix(self, summary: Dict) -> str:
        """
        Plot confusion matrix for each model.

        Args:
            summary: Summary statistics

        Returns:
            Path to the generated plot file
        """
        # Prepare data
        model_names = list(summary.keys())
        fig, axs = plt.subplots(1, len(model_names), figsize=(15, 5), sharey=True)

        if len(model_names) == 1:
            axs = [axs]  # Make iterable if only one model

        for i, model in enumerate(model_names):
            stats = summary[model]

            # Create confusion matrix
            cm = np.array([
                [stats["true_positive"], stats["false_negative"]],
                [stats["false_positive"], 0]  # TN is not tracked
            ])

            # Plot
            sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', ax=axs[i],
                       xticklabels=['Predicted Yes', 'Predicted No'],
                       yticklabels=['Actual Yes', 'Actual No'])

            axs[i].set_title(f"{model}")

        plt.tight_layout()

        # Save plot
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        file_path = self.results_dir / f"confusion_matrix_{timestamp}.png"
        plt.savefig(file_path, dpi=300)
        plt.close()

        return str(file_path)

    def _plot_f1_by_category(self) -> str:
        """
        Plot F1 scores by vulnerability category.

        Returns:
            Path to the generated plot file
        """
        # Prepare data
        categories = []
        model_f1_scores = defaultdict(list)

        for model, results in self.results.items():
            for item_key, item_result in results.items():
                if isinstance(item_key, str) and item_key in CATEGORIES:
                    if item_key not in categories:
                        categories.append(item_key)
                    model_f1_scores[model].append({
                        "category": item_key,
                        "f1_score": item_result.get("f1_score", 0)
                    })

        if not categories:
            logger.warning("No category data for F1 score visualization")
            return ""

        # Create DataFrame for seaborn
        plot_data = []
        for model, scores in model_f1_scores.items():
            for score_item in scores:
                plot_data.append({
                    "Model": model,
                    "Category": score_item["category"],
                    "F1 Score": score_item["f1_score"]
                })

        df = pd.DataFrame(plot_data)

        # Create plot
        plt.figure(figsize=(14, 7))
        chart = sns.barplot(x="Category", y="F1 Score", hue="Model", data=df)
        plt.title("F1 Score by Vulnerability Category")
        plt.xticks(rotation=45, ha='right')
        plt.ylim(0, 1.1)
        plt.legend(title="Model", bbox_to_anchor=(1.05, 1), loc='upper left')
        plt.tight_layout()

        # Save plot
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        file_path = self.results_dir / f"f1_by_category_{timestamp}.png"
        plt.savefig(file_path, dpi=300)
        plt.close()

        return str(file_path)

async def main():
    """Main function to run the benchmark evaluator."""
    parser = argparse.ArgumentParser(description="Benchmark Evaluator for Smart Contract Vulnerability Detection")

    parser.add_argument("--uploads-dir", type=str, default=UPLOADS_DIR,
                       help="Directory containing generated reports")
    parser.add_argument("--benchmark-data-dir", type=str, default=BENCHMARK_DATA_DIR,
                       help="Directory containing benchmark data with errors")
    parser.add_argument("--benchmark-no-errors-dir", type=str, default=BENCHMARK_DATA_NO_ERRORS_DIR,
                       help="Directory containing benchmark data with no errors")
    parser.add_argument("--results-dir", type=str, default=BENCHMARK_RESULTS_DIR,
                       help="Directory to save evaluation results")
    parser.add_argument("--report-dirs", type=str, nargs="+",
                       help="Specific report directories to evaluate")
    parser.add_argument("--use-llm", action="store_true", default=True,
                       help="Use LLM-based evaluation (default: True)")
    parser.add_argument("--no-llm", action="store_false", dest="use_llm",
                       help="Disable LLM-based evaluation")
    parser.add_argument("--eval-model", type=str, default=EVAL_MODEL,
                       help=f"LLM model to use for evaluation (default: {EVAL_MODEL})")
    parser.add_argument("--max-workers", type=int, default=5,
                       help="Maximum number of parallel workers (default: 5)")

    args = parser.parse_args()

    logger.info(f"Starting benchmark evaluation with settings: {args}")

    evaluator = BenchmarkEvaluator(
        uploads_dir=args.uploads_dir,
        benchmark_data_dir=args.benchmark_data_dir,
        benchmark_no_errors_dir=args.benchmark_no_errors_dir,
        results_dir=args.results_dir,
        use_llm=args.use_llm,
        eval_model=args.eval_model,
        max_workers=args.max_workers
    )

    # Run evaluation
    start_time = time.time()
    results = await evaluator.evaluate_all(args.report_dirs)
    end_time = time.time()

    logger.info(f"Evaluation completed in {end_time - start_time:.2f} seconds")

    # Save results
    results_file = evaluator.save_results()
    if results_file:
        logger.info(f"Results saved to {results_file}")

    # Generate visualizations
    viz_files = evaluator.generate_visualizations()
    if viz_files:
        logger.info(f"Generated {len(viz_files)} visualization files")
        for f in viz_files:
            logger.info(f"  - {f}")

    # Print summary
    summary = evaluator.generate_summary()
    logger.info("=== EVALUATION SUMMARY ===")
    for model, stats in summary.items():
        logger.info(f"\nModel: {model}")
        logger.info(f"Average Precision: {stats['avg_precision']:.4f}")
        logger.info(f"Average Recall: {stats['avg_recall']:.4f}")
        logger.info(f"Average F1 Score: {stats['avg_f1_score']:.4f}")
        logger.info(f"Macro F1 Score: {stats['macro_f1']:.4f}")
        logger.info(f"Total Reports: {stats['with_errors_count'] + stats['no_errors_count']}")
        logger.info(f"Evaluation Methods: {dict(stats['evaluation_methods'])}")

if __name__ == "__main__":
    asyncio.run(main())
