#!/usr/bin/env python3
"""
Exploit Success Rate Evaluator

This script evaluates the exploit generation capabilities of the smart contract analysis system.
It runs the analysis system against vulnerable contracts and measures:
1. How many vulnerabilities were detected
2. How many detected vulnerabilities had exploit scripts generated
3. How many of these exploits successfully executed

These metrics are used to calculate the overall exploit success rate.
"""

import os
import re
import json
import time
import asyncio
import argparse
import subprocess
from pathlib import Path
from typing import Dict, List, Tuple, Optional, Any
import logging
from datetime import datetime
import matplotlib.pyplot as plt
import seaborn as sns
import pandas as pd
from collections import defaultdict

# Configure logging
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler("exploit_evaluation.log"),
        logging.StreamHandler()
    ]
)
logger = logging.getLogger("exploit_evaluator")

# Constants
BENCHMARK_DATA_DIR = "benchmark_data"
RESULTS_DIR = "benchmark_results"

# Categories for benchmark contracts - based on directory structure in benchmark_data/contracts/with_errors
CATEGORIES = [
    "access_control",
    "arithmetic_security",
    "boundary_condition",
    "cryptoeconomic_security",
    "data_structure_security",
    "gas_security",
    "privacy_crypto_security"
]

class ExploitSuccessEvaluator:
    """Evaluates exploit generation and execution success rate."""

    def __init__(self,
                 benchmark_data_dir: str = BENCHMARK_DATA_DIR,
                 results_dir: str = RESULTS_DIR,
                 max_workers: int = 4):
        """
        Initialize the exploit success evaluator.

        Args:
            benchmark_data_dir: Directory containing benchmark data with vulnerable contracts
            results_dir: Directory to save evaluation results
            max_workers: Maximum number of parallel workers
        """
        self.benchmark_data_dir = Path(benchmark_data_dir)
        self.results_dir = Path(results_dir)
        self.max_workers = max_workers

        # Check if required directories exist
        self._check_directories()

        # Create results directory if it doesn't exist
        os.makedirs(self.results_dir, exist_ok=True)

        # Store evaluation results
        self.results = {}

    def _check_directories(self):
        """Check if required directories exist and create them if necessary."""
        # Check benchmark data directory
        if not os.path.exists(self.benchmark_data_dir):
            logger.error(f"Benchmark data directory {self.benchmark_data_dir} does not exist")
            logger.error("Please ensure the benchmark data is properly set up")
            raise FileNotFoundError(f"Benchmark data directory {self.benchmark_data_dir} not found")

        # Check for contracts directory with vulnerabilities
        contracts_dir = self.benchmark_data_dir / "contracts"
        if not os.path.exists(contracts_dir):
            logger.error(f"Contracts directory {contracts_dir} does not exist")
            logger.error("Please ensure the benchmark data is properly set up")
            raise FileNotFoundError(f"Contracts directory {contracts_dir} not found")

        # Check for with_errors directory
        with_errors_dir = contracts_dir / "with_errors"
        if not os.path.exists(with_errors_dir):
            logger.error(f"With errors directory {with_errors_dir} does not exist")
            logger.error("Please ensure the benchmark data is properly set up")
            raise FileNotFoundError(f"With errors directory {with_errors_dir} not found")

        # Check that at least some vulnerability categories exist
        found_categories = False
        for category in CATEGORIES:
            if os.path.exists(with_errors_dir / category):
                found_categories = True
                break

        if not found_categories:
            logger.error(f"No vulnerability categories found in {with_errors_dir}")
            logger.error("Please ensure the benchmark data is properly set up")
            raise FileNotFoundError(f"No vulnerability categories found in {with_errors_dir}")

        # Create results directory
        os.makedirs(self.results_dir, exist_ok=True)

    async def evaluate_exploits(self,
                         model_name: str = "o3-mini",
                         use_rag: bool = True,
                         categories: List[str] = None) -> Dict:
        """
        Evaluate exploit success rate for a specific model configuration.

        Args:
            model_name: Name of the model to use for analysis
            use_rag: Whether to use RAG for analysis
            categories: List of vulnerability categories to evaluate (if None, all categories)

        Returns:
            Dictionary containing evaluation results
        """
        # If no categories provided, use all
        if categories is None:
            categories = CATEGORIES

        logger.info(f"Evaluating exploit success for model {model_name} (RAG: {use_rag})")

        # Process each vulnerability category
        all_results = {}

        # Track overall stats
        total_contracts = 0
        vulnerabilities_detected = 0
        exploits_generated = 0
        exploits_successful = 0

        for category in categories:
            logger.info(f"Processing category: {category}")

            # Get the vulnerable contract file for this category
            contract_path = self._get_contract_path(category)
            if not contract_path:
                logger.warning(f"Contract file not found for category: {category}")
                continue

            total_contracts += 1

            # Run the system on this contract with the specific configuration
            result = await self._run_system_on_contract(contract_path, model_name, use_rag)

            # Add to category results
            all_results[category] = result

            # Update overall stats
            if result.get("vulnerability_detected", False):
                vulnerabilities_detected += 1

            if result.get("exploit_generated", False):
                exploits_generated += 1

            if result.get("exploit_successful", False):
                exploits_successful += 1

        # Calculate success rates
        detection_rate = vulnerabilities_detected / total_contracts if total_contracts > 0 else 0
        generation_rate = exploits_generated / vulnerabilities_detected if vulnerabilities_detected > 0 else 0
        success_rate = exploits_successful / exploits_generated if exploits_generated > 0 else 0
        overall_success_rate = exploits_successful / total_contracts if total_contracts > 0 else 0

        # Prepare overall results
        config_name = f"{model_name}_rag-{'on' if use_rag else 'off'}"
        self.results[config_name] = {
            "model": model_name,
            "rag": use_rag,
            "total_contracts": total_contracts,
            "vulnerabilities_detected": vulnerabilities_detected,
            "exploits_generated": exploits_generated,
            "exploits_successful": exploits_successful,
            "detection_rate": detection_rate,
            "generation_rate": generation_rate,
            "success_rate": success_rate,
            "overall_success_rate": overall_success_rate,
            "category_results": all_results
        }

        return self.results[config_name]

    def _get_contract_path(self, category: str) -> Optional[str]:
        """
        Get the path to the vulnerable contract file for a specific category.

        Args:
            category: Vulnerability category

        Returns:
            Path to the contract file or None if not found
        """
        # Try to find a .sol file in the category directory within with_errors
        category_dir = self.benchmark_data_dir / "contracts" / "with_errors" / category
        if not os.path.exists(category_dir):
            logger.warning(f"Category directory not found: {category_dir}")
            return None

        # Look for .sol files
        sol_files = list(category_dir.glob("*.sol"))

        if not sol_files:
            logger.warning(f"No .sol files found in {category_dir}")
            return None

        # Choose a representative contract from the category
        # Some categories have a lot of files, so we'll pick one that seems representative
        # Prioritize files based on some common patterns

        # First, look for files that contain the category name in their filename
        for file in sol_files:
            if category.lower() in file.name.lower():
                logger.info(f"Selected representative contract for {category}: {file.name}")
                return str(file)

        # If no such file, prefer simpler examples as they're more likely to work well
        # Sort by file size - smaller files are usually simpler
        sol_files.sort(key=lambda f: os.path.getsize(f))

        # Return the smallest file that's not just an interface or library
        for file in sol_files:
            with open(file, 'r') as f:
                content = f.read().lower()
                # Skip files that are just interfaces or libraries
                if 'contract ' in content and not ('library ' in content and len(content) < 1000):
                    logger.info(f"Selected representative contract for {category}: {file.name}")
                    return str(file)

        # If we get here, just return the first file
        logger.info(f"Selected first available contract for {category}: {sol_files[0].name}")
        return str(sol_files[0])

    async def _run_system_on_contract(self, contract_path: str, model_name: str, use_rag: bool) -> Dict:
        """
        Run the analysis system on a contract with the specified configuration.

        Args:
            contract_path: Path to the contract file
            model_name: Model to use for analysis
            use_rag: Whether to use RAG for analysis

        Returns:
            Dictionary containing results
        """
        logger.info(f"Running system on contract: {contract_path} with model: {model_name}, RAG: {use_rag}")

        # Build command to run the main.py script with the specified configuration
        python_executable = ".venv/bin/python"
        cmd = [
            python_executable, "main.py",
            "--contract", contract_path,
            "--analyzer-model", model_name
        ]

        # Add RAG option
        if not use_rag:
            cmd.append("--no-rag")

        # Default result in case of error
        default_result = {
            "vulnerability_detected": False,
            "exploit_generated": False,
            "exploit_successful": False,
            "details": "System execution failed"
        }

        try:
            # Run the command
            logger.info(f"Executing command: {' '.join(cmd)}")

            # Run with a timeout
            process = await asyncio.create_subprocess_exec(
                *cmd,
                stdout=asyncio.subprocess.PIPE,
                stderr=asyncio.subprocess.PIPE
            )

            # Wait for the process to complete (with timeout)
            try:
                stdout, stderr = await asyncio.wait_for(process.communicate(), timeout=1800)  # 30 min timeout
                stdout_text = stdout.decode('utf-8')
                stderr_text = stderr.decode('utf-8')
            except asyncio.TimeoutError:
                logger.error(f"Command timed out after 30 minutes")
                process.kill()
                return {**default_result, "details": "Execution timed out after 30 minutes"}

            # Check if the process succeeded
            if process.returncode != 0:
                logger.error(f"Command failed with exit code {process.returncode}")
                logger.error(f"Stderr: {stderr_text}")
                return {**default_result, "details": f"Process failed with exit code {process.returncode}"}

            # Log some output for debugging
            logger.info(f"Command completed successfully. Output preview:")
            for line in stdout_text.splitlines()[:20]:  # Log first 20 lines for debugging
                if ("Vulnerability" in line or "PoC" in line or "Execution" in line or
                    "[PASS]" in line or "[FAIL]" in line):
                    logger.info(f"-> {line}")

            # Parse the results from terminal output
            result = self._parse_results(stdout_text, stderr_text)

            return result

        except Exception as e:
            logger.error(f"Error running system on contract {contract_path}: {e}")
            return {**default_result, "details": str(e)}

    def _parse_results(self, stdout: str, stderr: str) -> Dict:
        """
        Parse the output of the analysis system to extract exploit success information.

        Args:
            stdout: Standard output of the command
            stderr: Standard error output of the command

        Returns:
            Dictionary containing parsed results
        """
        # Initialize result
        result = {
            "vulnerability_detected": False,
            "exploit_generated": False,
            "exploit_successful": False,
            "vulnerabilities": [],
            "exploits": [],
            "output": stdout,
            "error": stderr,
            "details": "No vulnerabilities detected"
        }

        # Check if any vulnerabilities were detected
        if "Found 0 potential vulnerabilities" in stdout:
            return result

        # Extract number of vulnerabilities
        vulnerability_count_match = re.search(r"Found (\d+) potential vulnerabilities", stdout)
        if vulnerability_count_match:
            vulnerability_count = int(vulnerability_count_match.group(1))
            result["vulnerability_detected"] = vulnerability_count > 0
            result["details"] = f"Detected {vulnerability_count} potential vulnerabilities"

        # Extract vulnerability information
        vulnerability_sections = re.findall(r"Vulnerability #\d+:(.*?)(?=Vulnerability #\d+:|Generated Proof of Concepts|$)", stdout, re.DOTALL)
        for section in vulnerability_sections:
            # Extract vulnerability type
            type_match = re.search(r"Vulnerability #\d+: ([\w\s-]+)", section)
            if type_match:
                vuln_type = type_match.group(1).strip()
                result["vulnerabilities"].append(vuln_type)

        # Check if any PoCs were generated
        if "No PoCs were generated" in stdout:
            result["exploit_generated"] = False
            result["details"] += ", but no exploits were generated"
            return result

        # Extract number of PoCs
        poc_count_match = re.search(r"Generated (\d+) PoCs", stdout)
        if poc_count_match:
            poc_count = int(poc_count_match.group(1))
            result["exploit_generated"] = poc_count > 0
            result["details"] += f", generated {poc_count} exploits"

        # Look for execution results in the output
        # This is the most reliable way to determine if exploits were successful
        successful_exploits = 0
        failed_exploits = 0

        # First check for Execution: SUCCESS/FAILED lines
        for line in stdout.splitlines():
            if "Execution: SUCCESS" in line:
                successful_exploits += 1
                result["exploit_successful"] = True
            elif "Execution: FAILED" in line:
                failed_exploits += 1

        # Also look for Forge test output patterns
        # Forge outputs [PASS] for successful tests
        pass_matches = re.findall(r"\[PASS\]\s+testExploit\(\)", stdout)
        successful_exploits += len(pass_matches)

        # Forge outputs [FAIL] for failed tests
        fail_matches = re.findall(r"\[FAIL\]\s+testExploit\(\)", stdout)
        failed_exploits += len(fail_matches)

        # Look for balance changes that indicate successful exploitation
        balance_pattern = r"Attacker ETH Balance Before.*?(\d+).*?Attacker ETH Balance After.*?(\d+)"
        balance_matches = re.findall(balance_pattern, stdout, re.DOTALL)

        for before, after in balance_matches:
            # If balance increased, it's a successful exploit
            if int(after) > int(before):
                result["exploit_successful"] = True

        if successful_exploits > 0:
            result["exploit_successful"] = True
            if result["exploit_generated"]:
                result["details"] += f", {successful_exploits} executed successfully"

        # Extract exploit filenames
        for line in stdout.splitlines():
            if "File:" in line and ".sol" in line:
                # Extract exploit file name
                exploit_file_match = re.search(r"File: ([\w\.\/_-]+\.sol)", line)
                if exploit_file_match:
                    exploit_file = exploit_file_match.group(1)
                    if exploit_file not in result["exploits"]:
                        result["exploits"].append(exploit_file)

        return result

    def _parse_json_results(self, json_results: Dict) -> Dict:
        """
        Parse the JSON results file to extract additional exploit success information.

        Args:
            json_results: JSON results from the analysis system

        Returns:
            Dictionary containing parsed results
        """
        result = {
            "vulnerability_detected": False,
            "exploit_generated": False,
            "exploit_successful": False,
            "vulnerabilities": [],
            "exploits": [],
            "details": "No JSON results"
        }

        # Check if there are any vulnerabilities
        if "rechecked_vulnerabilities" in json_results:
            vulnerabilities = json_results["rechecked_vulnerabilities"]
            result["vulnerability_detected"] = len(vulnerabilities) > 0
            result["vulnerabilities"] = [vuln.get("vulnerability_type", "Unknown") for vuln in vulnerabilities]

        # Check if there are any PoCs
        if "generated_pocs" in json_results:
            pocs = json_results["generated_pocs"]
            result["exploit_generated"] = len(pocs) > 0

            # Count successful exploits
            successful_exploits = 0
            for poc in pocs:
                if "poc_data" in poc and "execution_results" in poc["poc_data"]:
                    execution_results = poc["poc_data"]["execution_results"]
                    if execution_results.get("success", False):
                        successful_exploits += 1
                        result["exploit_successful"] = True

                # Extract exploit file info
                if "poc_data" in poc and "exploit_file" in poc["poc_data"]:
                    exploit_file = poc["poc_data"]["exploit_file"]
                    if exploit_file not in result["exploits"]:
                        result["exploits"].append(exploit_file)

            # Update details
            result["details"] = f"Detected {len(vulnerabilities)} vulnerabilities, generated {len(pocs)} exploits, {successful_exploits} executed successfully"

        return result

    def save_results(self, filename: str = None) -> str:
        """
        Save evaluation results to a JSON file.

        Args:
            filename: Name of the file to save results to (without extension)

        Returns:
            Path to the saved file
        """
        if not self.results:
            logger.warning("No results to save")
            return ""

        # Generate filename based on timestamp if not provided
        if not filename:
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            filename = f"exploit_results_{timestamp}"

        # Ensure .json extension
        if not filename.endswith(".json"):
            filename += ".json"

        # Create path in results directory
        file_path = self.results_dir / filename

        # Save results to JSON file
        try:
            with open(file_path, 'w', encoding='utf-8') as f:
                json.dump(self.results, f, indent=2)
            logger.info(f"Results saved to {file_path}")
            return str(file_path)
        except Exception as e:
            logger.error(f"Error saving results to {file_path}: {e}")
            return ""

    def generate_visualizations(self) -> List[str]:
        """
        Generate visualizations of the exploit success rates.

        Returns:
            List of paths to generated visualization files
        """
        if not self.results:
            logger.warning("No results to visualize")
            return []

        # Prepare data for visualizations
        viz_files = []

        # 1. Exploit Success Rate Comparison
        success_rate_chart = self._plot_success_rate_comparison()
        if success_rate_chart:
            viz_files.append(success_rate_chart)

        # 2. Exploit Funnel Chart
        funnel_chart = self._plot_exploit_funnel()
        if funnel_chart:
            viz_files.append(funnel_chart)

        # 3. Success Rate by Category
        category_chart = self._plot_success_by_category()
        if category_chart:
            viz_files.append(category_chart)

        return viz_files

    def _plot_success_rate_comparison(self) -> str:
        """
        Plot comparison of exploit success rates across different configurations.

        Returns:
            Path to the generated plot
        """
        # Prepare data
        models = []
        detection_rates = []
        generation_rates = []
        success_rates = []
        overall_rates = []

        for config_name, results in self.results.items():
            models.append(config_name)
            detection_rates.append(results.get("detection_rate", 0) * 100)
            generation_rates.append(results.get("generation_rate", 0) * 100)
            success_rates.append(results.get("success_rate", 0) * 100)
            overall_rates.append(results.get("overall_success_rate", 0) * 100)

        if not models:
            return ""

        # Create DataFrame
        df = pd.DataFrame({
            'Model': models,
            'Detection Rate (%)': detection_rates,
            'Generation Rate (%)': generation_rates,
            'Success Rate (%)': success_rates,
            'Overall Success (%)': overall_rates
        })

        # Melt for easier plotting
        df_melted = pd.melt(df, id_vars=['Model'], var_name='Metric', value_name='Percentage')

        # Create plot
        plt.figure(figsize=(12, 8))
        ax = sns.barplot(x='Model', y='Percentage', hue='Metric', data=df_melted)

        # Customize the plot
        plt.title('Exploit Success Metrics by Model Configuration', fontsize=16)
        plt.xlabel('Model Configuration', fontsize=14)
        plt.ylabel('Percentage (%)', fontsize=14)
        plt.ylim(0, 110)  # Leave room for error bars
        plt.xticks(rotation=45, ha='right')
        plt.legend(title='Metric', bbox_to_anchor=(1.05, 1), loc='upper left')

        # Add value labels on bars
        for p in ax.patches:
            ax.annotate(f'{p.get_height():.1f}%',
                        (p.get_x() + p.get_width() / 2., p.get_height()),
                        ha = 'center', va = 'bottom',
                        xytext = (0, 5), textcoords = 'offset points')

        plt.tight_layout()

        # Save the plot
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        file_path = self.results_dir / f"exploit_success_comparison_{timestamp}.png"
        plt.savefig(file_path, dpi=300, bbox_inches='tight')
        plt.close()

        return str(file_path)

    def _plot_exploit_funnel(self) -> str:
        """
        Plot funnel chart showing how many contracts go through each stage.

        Returns:
            Path to the generated plot
        """
        # Create subplots - one per configuration
        config_count = len(self.results)
        if config_count == 0:
            return ""

        # Calculate grid size
        cols = min(2, config_count)
        rows = (config_count + cols - 1) // cols

        plt.figure(figsize=(15, 5 * rows))

        # Sort configurations by name for consistent display
        sorted_configs = sorted(self.results.items())

        # Create funnel chart for each configuration
        for i, (config_name, results) in enumerate(sorted_configs):
            ax = plt.subplot(rows, cols, i + 1)

            # Get funnel data
            stages = ['Total Contracts', 'Vulnerabilities Detected', 'Exploits Generated', 'Successful Exploits']
            values = [
                results.get('total_contracts', 0),
                results.get('vulnerabilities_detected', 0),
                results.get('exploits_generated', 0),
                results.get('exploits_successful', 0)
            ]

            # Create funnel chart (using a horizontal bar chart)
            # Sort in reverse order for funnel effect
            colors = ['#5DA5DA', '#FAA43A', '#60BD68', '#F17CB0']
            bars = ax.barh(stages, values, color=colors, alpha=0.8)

            # Add data labels
            for bar in bars:
                width = bar.get_width()
                ax.text(width + 0.3, bar.get_y() + bar.get_height()/2,
                        f'{int(width)}', ha='left', va='center')

            # Add percentages as annotations
            if values[0] > 0:  # Prevent division by zero
                ax.text(values[1] + 0.3, stages[1], f'({values[1]/values[0]*100:.1f}%)', ha='left', va='center', alpha=0.7)
            if values[1] > 0:
                ax.text(values[2] + 0.3, stages[2], f'({values[2]/values[1]*100:.1f}%)', ha='left', va='center', alpha=0.7)
            if values[2] > 0:
                ax.text(values[3] + 0.3, stages[3], f'({values[3]/values[2]*100:.1f}%)', ha='left', va='center', alpha=0.7)

            # Set title and other properties
            ax.set_title(f'Exploit Funnel: {config_name}', fontsize=14)
            ax.set_xlabel('Number of Contracts', fontsize=12)
            ax.grid(axis='x', alpha=0.3)

            # Set a reasonable x-axis limit
            max_val = max(values)
            ax.set_xlim(0, max_val * 1.2 + 1)

        plt.tight_layout()

        # Save the plot
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        file_path = self.results_dir / f"exploit_funnel_{timestamp}.png"
        plt.savefig(file_path, dpi=300, bbox_inches='tight')
        plt.close()

        return str(file_path)

    def _plot_success_by_category(self) -> str:
        """
        Plot exploit success rate by vulnerability category.

        Returns:
            Path to the generated plot
        """
        # Prepare data
        plot_data = []

        for config_name, results in self.results.items():
            category_results = results.get("category_results", {})

            for category, cat_result in category_results.items():
                # Calculate success for this category
                detected = cat_result.get("vulnerability_detected", False)
                generated = cat_result.get("exploit_generated", False)
                successful = cat_result.get("exploit_successful", False)

                plot_data.append({
                    "Configuration": config_name,
                    "Category": category,
                    "Detected": 1 if detected else 0,
                    "Generated": 1 if generated else 0,
                    "Successful": 1 if successful else 0
                })

        if not plot_data:
            return ""

        # Convert to DataFrame
        df = pd.DataFrame(plot_data)

        # Create a pivot table for easier plotting
        pivot_df = df.pivot_table(
            index='Category',
            columns='Configuration',
            values='Successful',
            aggfunc='sum'
        ).fillna(0)

        # Create the heatmap
        plt.figure(figsize=(12, 8))
        sns.heatmap(pivot_df, annot=True, cmap="YlGnBu", cbar_kws={'label': 'Successful Exploits'})

        plt.title('Successful Exploits by Category and Configuration', fontsize=16)
        plt.ylabel('Vulnerability Category', fontsize=14)
        plt.xlabel('Model Configuration', fontsize=14)
        plt.tight_layout()

        # Save the plot
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        file_path = self.results_dir / f"exploit_by_category_{timestamp}.png"
        plt.savefig(file_path, dpi=300, bbox_inches='tight')
        plt.close()

        return str(file_path)

async def main():
    """Main function to run the exploit success evaluator."""
    parser = argparse.ArgumentParser(description="Exploit Success Rate Evaluator")

    parser.add_argument("--benchmark-data-dir", type=str, default=BENCHMARK_DATA_DIR,
                       help="Directory containing benchmark data")
    parser.add_argument("--results-dir", type=str, default=RESULTS_DIR,
                       help="Directory to save evaluation results")
    parser.add_argument("--max-workers", type=int, default=4,
                       help="Maximum number of parallel workers")
    parser.add_argument("--categories", type=str, nargs="+",
                       help="Specific vulnerability categories to evaluate")
    parser.add_argument("--models", type=str, nargs="+", default=["o3-mini"],
                       help="Models to evaluate")
    parser.add_argument("--rag", type=str, choices=["on", "off", "both"], default="both",
                       help="RAG configuration to use ('on', 'off', or 'both')")
    parser.add_argument("--load-results", type=str,
                       help="Path to an existing results JSON file to visualize without re-running evaluation")
    parser.add_argument("--example-contract", type=str,
                       help="Run evaluation on a single contract file for testing")

    args = parser.parse_args()

    # Initialize the evaluator
    evaluator = ExploitSuccessEvaluator(
        benchmark_data_dir=args.benchmark_data_dir,
        results_dir=args.results_dir,
        max_workers=args.max_workers
    )

    # Check if we should load existing results
    if args.load_results:
        if not os.path.exists(args.load_results):
            logger.error(f"Results file not found: {args.load_results}")
            return

        try:
            logger.info(f"Loading existing results from {args.load_results}")
            with open(args.load_results, 'r', encoding='utf-8') as f:
                evaluator.results = json.load(f)

            # Generate visualizations only
            viz_files = evaluator.generate_visualizations()
            if viz_files:
                logger.info(f"Generated {len(viz_files)} visualization files:")
                for file_path in viz_files:
                    logger.info(f"  - {file_path}")

            # Print summary
            logger.info("=== EXPLOIT SUCCESS EVALUATION SUMMARY ===")
            for config_name, results in evaluator.results.items():
                logger.info(f"\nConfiguration: {config_name}")
                logger.info(f"Total contracts tested: {results.get('total_contracts', 0)}")
                logger.info(f"Vulnerabilities detected: {results.get('vulnerabilities_detected', 0)} ({results.get('detection_rate', 0)*100:.1f}%)")
                logger.info(f"Exploits generated: {results.get('exploits_generated', 0)} ({results.get('generation_rate', 0)*100:.1f}%)")
                logger.info(f"Successful exploits: {results.get('exploits_successful', 0)} ({results.get('success_rate', 0)*100:.1f}%)")
                logger.info(f"Overall success rate: {results.get('overall_success_rate', 0)*100:.1f}%")

            return
        except Exception as e:
            logger.error(f"Error loading results from {args.load_results}: {e}")
            return

    # Special case for testing with a single example contract
    if args.example_contract:
        if not os.path.exists(args.example_contract):
            logger.error(f"Example contract file not found: {args.example_contract}")
            return

        logger.info(f"Running evaluation on example contract: {args.example_contract}")

        # Get the model from args.models
        model_name = args.models[0] if args.models else "o3-mini"

        # Use RAG based on args
        use_rag = True if args.rag in ["on", "both"] else False

        # Run the system on this contract manually
        result = await evaluator._run_system_on_contract(
            contract_path=args.example_contract,
            model_name=model_name,
            use_rag=use_rag
        )

        # Print detailed results for debugging
        logger.info("=== EXAMPLE CONTRACT EVALUATION RESULTS ===")
        logger.info(f"Contract: {args.example_contract}")
        logger.info(f"Model: {model_name} (RAG: {'on' if use_rag else 'off'})")
        logger.info(f"Vulnerability detected: {result['vulnerability_detected']}")
        if result["vulnerability_detected"]:
            logger.info(f"Vulnerabilities found: {', '.join(result['vulnerabilities'])}")

        logger.info(f"Exploit generated: {result['exploit_generated']}")
        if result["exploit_generated"]:
            logger.info(f"Exploits: {', '.join(result['exploits'])}")

        logger.info(f"Exploit successful: {result['exploit_successful']}")
        logger.info(f"Details: {result['details']}")

        # Save to results for visualization
        config_name = f"{model_name}_rag-{'on' if use_rag else 'off'}_example"
        evaluator.results[config_name] = {
            "model": model_name,
            "rag": use_rag,
            "total_contracts": 1,
            "vulnerabilities_detected": 1 if result["vulnerability_detected"] else 0,
            "exploits_generated": 1 if result["exploit_generated"] else 0,
            "exploits_successful": 1 if result["exploit_successful"] else 0,
            "detection_rate": 1 if result["vulnerability_detected"] else 0,
            "generation_rate": 1 if result["exploit_generated"] and result["vulnerability_detected"] else 0,
            "success_rate": 1 if result["exploit_successful"] and result["exploit_generated"] else 0,
            "overall_success_rate": 1 if result["exploit_successful"] else 0,
            "category_results": {
                "example": result
            }
        }

        # Save results
        results_file = evaluator.save_results()
        if results_file:
            logger.info(f"Results saved to {results_file}")

        return

    # For full benchmark evaluation:
    # Determine RAG configurations to test
    rag_configs = []
    if args.rag == "on" or args.rag == "both":
        rag_configs.append(True)
    if args.rag == "off" or args.rag == "both":
        rag_configs.append(False)

    # Run evaluations for all model and RAG combinations
    start_time = time.time()

    for model_name in args.models:
        for use_rag in rag_configs:
            try:
                results = await evaluator.evaluate_exploits(
                    model_name=model_name,
                    use_rag=use_rag,
                    categories=args.categories
                )

                logger.info(f"Completed evaluation for {model_name} (RAG: {'on' if use_rag else 'off'})")
                logger.info(f"Detected {results['vulnerabilities_detected']} vulnerabilities, generated {results['exploits_generated']} exploits, {results['exploits_successful']} successful")
                logger.info(f"Success rate: {results['success_rate']*100:.1f}%")
            except Exception as e:
                logger.error(f"Error evaluating {model_name} (RAG: {'on' if use_rag else 'off'}): {e}")

    end_time = time.time()
    logger.info(f"All evaluations completed in {end_time - start_time:.2f} seconds")

    # Save results
    results_file = evaluator.save_results()
    if results_file:
        logger.info(f"Results saved to {results_file}")

    # Generate visualizations
    viz_files = evaluator.generate_visualizations()
    if viz_files:
        logger.info(f"Generated {len(viz_files)} visualization files:")
        for file_path in viz_files:
            logger.info(f"  - {file_path}")

    # Print summary
    logger.info("=== EXPLOIT SUCCESS EVALUATION SUMMARY ===")
    for config_name, results in evaluator.results.items():
        logger.info(f"\nConfiguration: {config_name}")
        logger.info(f"Total contracts tested: {results.get('total_contracts', 0)}")
        logger.info(f"Vulnerabilities detected: {results.get('vulnerabilities_detected', 0)} ({results.get('detection_rate', 0)*100:.1f}%)")
        logger.info(f"Exploits generated: {results.get('exploits_generated', 0)} ({results.get('generation_rate', 0)*100:.1f}%)")
        logger.info(f"Successful exploits: {results.get('exploits_successful', 0)} ({results.get('success_rate', 0)*100:.1f}%)")
        logger.info(f"Overall success rate: {results.get('overall_success_rate', 0)*100:.1f}%")

if __name__ == "__main__":
    # Example usage:
    # Single contract test:
    # python exploit_success_evaluator.py --example-contract ./benchmark_data/contracts/with_errors/reentrancy/reentrancy_simple.sol --models o3-mini --rag on
    #
    # Full benchmark:
    # python exploit_success_evaluator.py --models o3-mini claude-3-7-sonnet-20240229 --rag both
    #
    # Specific categories:
    # python exploit_success_evaluator.py --models o3-mini --rag on --categories arithmetic_security cryptoeconomic_security

    asyncio.run(main())
